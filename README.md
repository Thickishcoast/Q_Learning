# Q_Learning

#You will work with a 4*4 board. Each of the 16 squares has a unique index. There are five special squares on the board. These five squares are the start, goal, forbidden, and wall squares. The remaining 11 squares are empty and ordinary. The starting square is fixed and always at square 2. The location of the two goals, forbidden and wall squares, are determined from the input. An agent has four possible actions: going to the north, east, south, and west. The board is bounded from the sides.
#The input to your program consists of four numbers, one character, and possibly an additional number [# # # X (#)]. The first four numbers show the location of the two goals, forbidden and wall squares, respectively. Figure 1 shows two possible inputs and the corresponding board configuration based on each of those inputs. The remaining items in the input determine the output format. The fourth item is either character “p” or “q,” and if it’s “q,” there will be an additional number at the end. Item “p” refers to printing the optimal policy (Π*), and “q” refers to the optimal Q-values (Q*). You can assume that the five special squares are distinct (non-overlapping).
#You should use Q-learning to calculate the best action for each square to reach the goal square. In the beginning, all of the Q-values are set to zero. The (hypothetical) agent should start from the S square. The agent iteratively updates the Q-values for each state by following the main formula
 #In this problem, the living reward for every action (each step) is r=-0.1. The discount rate is γ = 0.1, and the learning rate is α = 0.3. The reward for performing the exit action (the only available action) in both goal squares is +100, and for the forbidden square is -100. The agent cannot enter or pass through the wall square. After hitting the wall, the agent’s position will not be updated. It will remain in the same square and will keep getting a -0.1 reward every time it hits the wall. For the purpose of exploring the board, use an ε-greedy method with ε = 0.5. This means that with the probability ε, the agent acts randomly, and with the probability 1-ε, it acts on current policy. In order to have a similar random value, use 1 as the seed value of your random function.
#If needed, you can also set the maximum number of iterations to 100,000. After that, you can set ε= 0.
